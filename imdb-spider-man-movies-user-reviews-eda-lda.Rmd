---
output:
  html_document:
    theme: flatly
    highlight: pygments
    toc: true
    toc_depth: 3
    df_print: paged
    code_folding: show
---

```{r, include=FALSE}
library(tidyverse)
library(tidytext)
library(lubridate)
library(scales)
library(broom)
library(widyr)
library(igraph)
library(ggraph)
library(topicmodels)
library(patchwork)

theme_set(theme_bw(base_size = 12))
theme_update(panel.border = element_blank(),
             strip.background = element_rect(color = NA, fill = NA))

spider_man_colors <- c("#9d1a28", "#29329b", "#bf2431", "#3a76b1", "#8e98a2", "#c6989f", "#4d98e1", "#a0282f", "#2b2e5c")

options(scipen = 20)

knitr::opts_chunk$set(warning = FALSE,
                      message = FALSE,
                      fig.align = "center",
                      fig.width = 9,
                      fig.height = 7.5)

```


# Introduction

**Attention! This notebook contains spoilers about Spider-Man Movies.**

When a product influences me in a good or a bad way I almost always want to hear other people's experiences as well. This is certainly the case when it comes to Spider-Man for me. My first exposure to this hero was The Animated Series when I was a kid. Since then I tried to watch every Spider-Man media available. Through time I reacted differently to these media. Speaking specifically for movies, I loved and adored some of them. Some other movies couldn't satisfy me. I also realized that people have different tastes when it comes to Spider-Man movies. Some really like The Amazing Spider-Man series, I don't. Though I really liked seeing Andrew Garfield as Spider-Man.

After I learned a little bit about Text Analysis, I thought that analyzing this familiar topic would be a good practice. It would also quench my curiosity about other people's reactions the these movies. I will be following the book [Text Mining with R](https://www.tidytextmining.com/index.html) throughout this notebook. That's a very valuable resource if you want to learn more about Text Mining.

Here starting with some summary statistics of the review data.

```{r}
# Reading the Data
review_data <- read_csv("../input/spiderman-movies-imdb-reviews/imdb-spider-man-reviews.csv", show_col_types = FALSE)

# Helpfulness Variable
review_data <- review_data %>% 
  mutate(Helpfulness = Helpful_Vote / Total_Vote,
         Helpfulness = replace_na(Helpfulness, 0))

# Creating Review ID Numbers 
review_data <- review_data %>% 
  mutate(Review_ID = row_number())

# Parsing the Date Variable
review_data <- review_data %>% 
  mutate(Date = dmy(Date))

# Adding custom words to the stop word filter
stop_words <- stop_words %>% 
  bind_rows(
    tibble(
      word = c("spider", "spiderman", "man", "film", "movie", "movies"),
      lexicon = "custom"
    )
  )

# Creating Review Length Variable (The number of words.)
unigram_tokens <- review_data %>% 
  mutate(Review = str_to_lower(Review, locale = "en")) %>% 
  unnest_tokens(output = Word, input = Review, to_lower = FALSE)

review_data <- unigram_tokens %>% 
  group_by(Review_ID) %>% 
  summarise(Review_Length = n()) %>% 
  ungroup() %>% 
  inner_join(review_data)

skimr::skim(review_data)
```


# Non-Textual Analysis

Let's start the analysis with the non-textual properties of the reviews such as Rating, Helpfulness and Date. I also engineered the Review Length variable by counting the number of words that a review has.

## Univariate Analysis

We can see distributions and mean ratings for each Spider-Man movie on the bar-plot below. It is nice to see that my three all time favorite movies (Spider-Man 2, No Way Home, Into the Spider-Verse) were also rated highly by IMDB users. Spider-Man 3 was a huge disappointment for me and seeing that it is also the worst rated movie doesn't surprise me. The Amazing Spider-Man series and the first two movies of Marvel Cinematic Universe (Homecoming, Far From Home) never really moved me but I really liked the performances of both Andrew Garfield and Tom Holland.

```{r}
mean_ratings <- review_data %>% 
  group_by(Movie) %>% 
  summarise(Rating = mean(Rating, na.rm = TRUE))

review_data %>% 
  ggplot(aes(Rating)) +
  geom_bar(fill = "darkblue") +
  geom_vline(aes(xintercept = Rating, color = Movie), data = mean_ratings,
             show.legend = FALSE, size = 1.5)+
  geom_label(aes(label = round(Rating, digits = 2), y = Inf, x = Rating, color = Movie),
            data = mean_ratings, vjust = 1, hjust = 1, show.legend = FALSE, size = 6)+
  scale_x_continuous(n.breaks = 10)+
  scale_color_manual(values = spider_man_colors)+
  facet_wrap(~Movie, scales = "free")+
  labs(y = NULL, x = NULL, title = "Rating Distributions and Mean Ratings for each Movie")
```


The distributions might indicate the differences between reactions. For instance, ratings of Spider-Man 3 kind of look like they were uniformly distributed. Some people loved it, some people hated it and some other people thought that it was a mediocre movie. We can observe a similar distribution for The Amazing Spider-Man 2 as well. On the other hand, rating distributions of No way Home and Into the Spider-Verse are really skewed to the left with a very thin tail. People mostly loved these movies.


Helpfulness (Helpful Votes / Total Votes) of a review might give an idea about how review readers reacted to reviews. We can see that Spider-Man 2 averaged 58% helpfulness. Readers agreed many things there but we don't really know what kind of a review they mostly agreed. Maybe they found some criticisms constructive and valuable or maybe they just voted praising highly rated reviews helpful because they really loved the movie too. This question can certainly be answered but that requires more than one variable. Since this is an introductory univarite analysis we pass this question for now while keeping it in our minds.

```{r}
mean_helpfulness <- review_data %>% 
  filter(Total_Vote > 2) %>% 
  group_by(Movie) %>% 
  summarise(Helpfulness = mean(Helpfulness, na.rm = TRUE))

review_data %>% 
  filter(Total_Vote > 1) %>% 
  ggplot(aes(Helpfulness))+
  geom_histogram(color = "white", fill = "darkblue") +
  geom_vline(aes(xintercept = Helpfulness, color = Movie), data = mean_helpfulness,
             show.legend = FALSE, size = 1.5)+
  geom_label(aes(label = round(Helpfulness, digits = 2), y = Inf, x = Helpfulness, color = Movie),
             data = mean_helpfulness, show.legend = FALSE, vjust = 1, hjust = 1, size = 5)+
  scale_color_manual(values = spider_man_colors)+
  facet_wrap(~Movie, scales = "free")+
  labs(y = NULL, x = NULL, title = "Helpfulness Distributions and Mean Helpfulness for each Movie")
```


Review Length is an interesting variable. Means and Distributions can change a lot from movie to movie. I think Review Length might be used as an information metric, in other words the lengthier a review gets more detail it contains about the movie. For some movies like Spider-Man 3 or TASM reviewers really wanted to elaborate more and more. A strong majority of reviews have 100 words and more for these movies. On the other hand, Into the Spider-Verse had many short reviews. These reviews are mostly under 100 words.

```{r}
mean_review_length <- review_data %>% 
  group_by(Movie) %>% 
  summarise(Review_Length = mean(Review_Length))

review_data %>% 
  ggplot(aes(Review_Length))+
  geom_histogram(color = "white", fill = "darkblue")+
  geom_vline(aes(xintercept = Review_Length, color = Movie), data = mean_review_length,
             show.legend = FALSE, size = 1.5)+
  geom_label(aes(label = round(Review_Length), y = Inf, x = Review_Length, color = Movie),
             data = mean_review_length, show.legend = FALSE, vjust = 1, hjust = 0, size = 5)+
  facet_wrap(~Movie, scales = "free")+
  scale_x_log10()+
  scale_color_manual(values = spider_man_colors)+
  labs(y = NULL, x = NULL, title = "Review Length Distributions and Mean Length for each Movie",
       subtitle = "X-Axis is logarithmic scale")
```


## Bivariate Analysis

Now let's explore the relationships between the variables we analyzed above. Starting with Rating against Review Length we observe a weak non-linear relationship for almost every movie, an inverse U-Shape to be exact. I guess people don't want to elaborate more when they either love or hate the movie. From rating 4 to 8 we can observe that Review length is higher so I think these ratings naturally require more elaboration because they are somewhat two sided rather than only good or only bad. 

```{r}
review_data %>% 
  ggplot(aes(Rating, Review_Length, group = Rating))+
  geom_boxplot(color = "#29329b")+
  geom_smooth(aes(Rating, Review_Length), inherit.aes = F, size = 1.2, color = "#9d1a28")+
  facet_wrap(~Movie)+
  scale_x_continuous(n.breaks = 10)+
  scale_color_manual(values = spider_man_colors)+
  scale_y_log10()+
  labs(y = "Review Length", subtitle = "Y-Axis is logarithmic scale",
       title = "The Relationship between Review Length and Rating")
```

The relationship between Review Length and Review Helpfulness is plotted below. Except for Into the Spider-Verse and No Way Home, an inverse relationship between two variables can be observed. Shorter reviews seem to found more helpful than longer ones. To be clear, Spider-Man and Spider-Man 2 show a non-linear relationship but confidence bands are really wide at tails. They are high likely are getting influenced by outlier observations. 

```{r}
review_data %>% 
  filter(Total_Vote > 2) %>% 
  ggplot(aes(Review_Length, Helpfulness))+
  geom_point(alpha = 0.4, color = "#29329b")+
  geom_smooth(color = "#9d1a28", size = 1.2)+
  scale_x_log10()+
  facet_wrap(~Movie, scales = "free")+
  labs(x = "Review Length", subtitle = "X-Axis is logarithmic scale",
       title = "The Relationship between Review Helpfulness and Review Length")
```

Remember the question we asked when we were analyzing Helpfulness in the former section? Now we can get somewhat of an answer. It appears that people usually find high rated reviews helpful. This is very plain for Spider-Man and Spider-Man 2, TASM 2 and Far From Home. We can also observe that for some movies neutral rated reviews (4-6) are less helpful even compared to very low rated reviews. This is clear for Homecoming, Far From Home and No Way Home. 

```{r}
review_data %>% 
  filter(Total_Vote > 2) %>% 
  ggplot(aes(Rating, Helpfulness, group = Rating))+
  geom_boxplot(color = "#29329b")+
  geom_smooth(inherit.aes = FALSE, aes(Rating, Helpfulness), color = "#9d1a28", size = 1.2)+
  facet_wrap(~Movie)+
  scale_x_continuous(n.breaks = 10)+
  labs(title = "The Relationship between Review Helpfulness and Rating")
```

Lastly I want to plot all these variables against time. Additionally, since Spider-Man: No Way Home brought characters from both Original Spider-Man and The Amazing Spider-Man I want to see if something changed for reviews of the older movies after the release date of No Way Home.

Review Ratings through time is very interesting to look at. Except for Spider-Man 3 we can observe a strong slump for ratings little bit after the first reviews were published. Overtime, ratings start to normalize again and even get higher. I don't really have an actual idea about these slumps. Maybe the first wave of the reviewers are also enthusiastic fans so they give higher ratings. Spider-Man 3 is an interesting case because over time it gets better review rating scores. Maybe it couldn't meet the high expectations at first but then people realized that it wasn't a horrible movie. I also marked two time points on the plots. The first one is the birth of the internet meme "Bully Maguire" which is based on Spider-Man 3 and Tobey Maguire. Bully Maguire videos were watched over millions of times on YouTube and this definitely brings attention to the original trilogy especially. The second one is the release date of the movie No Way Home which made call backs to older Spider-Man Movies. I don't think YouTube exposure of Bully Maguire affected ratings over time. I don't think the release of No Way Home affected ratings either. Trends are looking very stable.

```{r}
time_points <- data.frame(
  labels = c("Bully Maguire", "No Way Home"),
  dates = c(ymd("2018-02-18"), ymd("2021-12-15"))
)

review_data %>% 
  ggplot(aes(Date, Rating))+
  geom_point(color = "#29329b", alpha = 0.4)+
  geom_smooth(color = "#9d1a28", size = 1.2)+
  geom_vline(data = time_points,
             mapping = aes(xintercept = dates, color = labels),
             size = 1.3)+
  scale_y_continuous(n.breaks = 10)+
  scale_x_date(date_minor_breaks = "year")+
  facet_wrap(~Movie, scales = "free")+
  labs(x = NULL, title = "Review Ratings over Time")+
  scale_color_manual(breaks = c("Bully Maguire", "No Way Home"),
                     values = c("Bully Maguire" = "#4d98e1", "No Way Home" = "#bf2431"))+
  theme(legend.position = "bottom",
        legend.title = element_blank())

```


Can we say that older reviews are usually longer? For a number of movies maybe we can say that. The original trilogy had some interesting influx of shorter reviews after about 2018. Can this be the Bully Maguire effect? We should definitely investigate this further when we analyze the bigrams.

```{r}
review_data %>% 
  ggplot(aes(Date, Review_Length))+
  geom_point(alpha = 0.4, color = "#29329b")+
  geom_smooth(color = "#9d1a28", size = 1.2)+
  geom_vline(data = time_points,
             mapping = aes(xintercept = dates, color = labels),
             size = 1.3)+
  facet_wrap(~Movie, scales = "free")+
  scale_x_date(date_minor_breaks = "years")+
  scale_y_log10()+
  labs(x = NULL, y = "Review Length", subtitle = "Y-Axis is logarithmic scale",
       title = "Review Lengths over Time")+
  scale_color_manual(breaks = c("Bully Maguire", "No Way Home"),
                     values = c("Bully Maguire" = "#4d98e1", "No Way Home" = "#bf2431"))+
  theme(legend.position = "bottom",
        legend.title = element_blank())
```

Lastly, let's also plot total votes a review gets and helpfulness of a review over time. Can we say that older reviews get more votes? For instance for Spider-Man and Spider-Man 2 we observe that there are too many old reviews which were voted zero times. Those observations pulled the smoother curve down to zero. This raises questions because other movies show the opposite. Older reviews were voted more. It makes sense because older reviews have more time to be voted. 

```{r}
review_data %>% 
  ggplot(aes(Date, Total_Vote+1))+
  geom_point(alpha = 0.4, color = "#29329b")+
  geom_smooth(color = "#9d1a28", size = 1.2)+
  geom_vline(data = time_points,
             mapping = aes(xintercept = dates, color = labels),
             size = 1.3)+
  facet_wrap(~Movie, scales = "free")+
  scale_y_log10()+
  labs(x = NULL, y = "Total Votes", subtitle = "Y-Axis is logarithmic scale",
       title = "Total Votes over Time")+
  scale_color_manual(breaks = c("Bully Maguire", "No Way Home"),
                     values = c("Bully Maguire" = "#4d98e1", "No Way Home" = "#bf2431"))+
  theme(legend.position = "bottom",
        legend.title = element_blank())
```

To plot review helpfulness (Helpful Votes divided by Total Votes) I filtered out reviews that were never voted because zero voted reviews are probably weren't even read at the first place. Except for Into the Spider-Verse and maybe Far From Home, we can observe that more recent reviews are more helpful.However, we should keep in mind that a total vote of one and a total votes of fifty aren't the same thing. We don't really know how many people read a particular review and decided to vote whether positively or not. Ignoring this would lead us wrong inferences.


```{r}
review_data %>% 
  filter(Total_Vote > 0) %>% 
  ggplot(aes(Date, Helpfulness))+
  geom_point(alpha = 0.4, color = "#29329b")+
  geom_smooth(color = "#9d1a28", size = 1.2)+
  geom_vline(data = time_points,
             mapping = aes(xintercept = dates, color = labels),
             size = 1.3)+
  facet_wrap(~Movie, scales = "free")+
  labs(x = NULL, title = "Review Helpfulness over Time")+
  scale_color_manual(breaks = c("Bully Maguire", "No Way Home"),
                     values = c("Bully Maguire" = "#4d98e1", "No Way Home" = "#bf2431"))+
  theme(legend.position = "bottom",
        legend.title = element_blank())
```


# Text Analysis

Finally we can start to analyze the reviews and reviews' titles' text. Firstly, we will see the most frequent unigrams, bigrams and trigrams for each movie. Then we will jump to the Term Frequency - Inverse Document Frequency analysis. We will try to identify the most important words and word couples. Lastly, we are going to play with Sentiment analysis in order to have an idea about in what manner people wrote their reviews.

## Unigrams

### Titles

After applying the stop words filter I plotted the most 20 frequent words for each movie. There are many things to unpack in that plot but unfortunately talking about every one bit would take a lot of time so I am going to pick some that I want to share. 

```{r, fig.height=11}
unigram_title_tokens <- review_data %>% 
  mutate(Title = str_to_lower(Title, locale = "en")) %>% 
  unnest_tokens(output = Word, input = Title, to_lower = F)


unigram_title_tokens %>% 
  anti_join(stop_words, by = c("Word" = "word")) %>% 
  group_by(Movie) %>% 
  count(Word, name = "Word_Count") %>% 
  slice_max(order_by = Word_Count, n = 20) %>% 
  ungroup() %>% 
  mutate(Word = reorder_within(Word, Word_Count, Movie)) %>% 
  ggplot(aes(Word_Count, Word, fill = Movie))+
  geom_col(show.legend = F)+
  scale_y_reordered()+
  facet_wrap(~Movie, scales = "free")+
  scale_fill_manual(values = spider_man_colors)+
  labs(y = NULL, x = NULL)
```

* Spider-Man 2, TASM 2 and Far From Home, we can see the word "*sequel*" on the plots so we can understand that, well, these movies were sequels. 

* We see many negative words such as "*disappointing, worst, terrible, mess, waste*" were used in the review titles of Spider-Man 3 and TASM 2. These movies weren't really liked by fans. However, keep in mind that Spider-Man 3 gave birth to a very beautiful meme "Bully Maguire".

* The word "*reboot*" was used for TASM and Homecoming since they came after the original trilogy.

* In Home Series, words "*marvel, mcu*" were used a lot. 

* No Way Home brought old Spider-Men, Peter Parkers and their villains into the movie. Thus we can see words such as "*fan, service, nostalgia*". 

### Reviews

Now let's apply the same thing to reviews. One interesting difference is that title unigrams usually described the movie itself and not what it contains. "Is it a sequel?, Is it terrible?" etc. On the opposite, now in reviews' unigrams we can see the names of characters and actor/actress names.

```{r, fig.height=11}
unigram_tokens %>% 
  anti_join(stop_words, by = c("Word"= "word")) %>% 
  group_by(Word, Movie) %>% 
  count(name = "Word_Count", sort = TRUE) %>% 
  ungroup() %>% 
  group_by(Movie) %>% 
  slice_max(order_by = Word_Count, n = 20) %>% 
  mutate(Word = reorder_within(Word, Word_Count, Movie)) %>% 
  ggplot(aes(Word_Count, Word, fill = Movie))+
  geom_col(show.legend = F)+
  facet_wrap(~Movie, scales = "free")+
  scale_fill_manual(values = spider_man_colors)+
  scale_y_reordered()+
  labs(y = NULL, x = NULL)
```

* The spider-man in the movie Into the Spider-Verse is Miles Morales that's why the word "*peter*" isn't the first or the second frequent word.

* "*green, goblin, doc, ock, sandman, venom, mysterio, vulture, lizard, electro*" are the villains of their respective movies.

* For MCU movies we can see words such as "*iron, tony, stark, endgame, strange*" since Marvel Cinematic Universe movies are interconnected to each other. We've seen Iron Man in Homecoming and Far From Home and Dr.Strange in No Way Home. Also Far From Home takes place after the Avengers: Endgame.

* Spider-Man has many love affairs and this can be derived from words such as "*love, mj, mary, jane, gwen, stacy*" 

* Words "*andrew, tobey*" are present in No Way Home too because Tobey Maguire and Andrew Garfield played the Spider-Man in the movie too.

## Bigrams

Unigrams are cool but usually they don't give many information about the context. In the unigram analysis we saw words that were separated but they get different meanings together. For instance, "Green Goblin" was divided into two words. Not knowing about the context someone might think that there is green something in the movie, maybe Spider-Man had a green suit there who knows?

To delve into the reviews a little bit more we also need to analyse bigrams(word couples). Starting with titles.

### Titles

```{r, fig.height=11}
title_bigrams <- review_data %>% 
  mutate(Title = str_to_lower(Title, locale = "en")) %>% 
  unnest_tokens(output = Bigram, input = Title, to_lower = F, token = "ngrams", n = 2) 
  

filtered_title_bigrams <- title_bigrams %>% 
  separate(Bigram, into = c("first", "second"), sep = " ") %>% 
  anti_join(stop_words, by = c("first" = "word")) %>% 
  anti_join(stop_words, by = c("second" = "word")) %>% 
  filter(!is.na(first), !is.na(second)) %>% 
  unite(col = "Bigram", first, second, sep = " ")

filtered_title_bigrams %>% 
  group_by(Bigram, Movie) %>% 
  count(name = "Bigram_Count") %>% 
  ungroup() %>% 
  group_by(Movie) %>% 
  slice_max(order_by = Bigram_Count, n = 20,  with_ties = FALSE) %>% 
  mutate(Bigram = reorder_within(Bigram, Bigram_Count, Movie)) %>% 
  ggplot(aes(Bigram_Count, Bigram, fill = Movie))+
  geom_col(show.legend = FALSE)+
  scale_fill_manual(values = spider_man_colors)+
  scale_y_reordered()+
  facet_wrap(~Movie, scales = "free", strip.position = "right")+
  labs(y = NULL, x = NULL)
  
```

Now we can see that some individual words that we analyzed above came together. Turns out words *green* and *goblin* were used together frequently. Director names, actor names and character names came together as well. Title bigrams mostly seem to be describing the reviewers' reactions to the movies. 

* For TASM we can see word couples such as "*tobey maguire, sam raimi, original trilogy*". Naturally this reboot was being compared to the movies came before it. Bigrams such as "*unnecessary reboot, amazingly unnecessary*" might indicate that some fans didn't really like the new reboot. On the contrast there is a word couple "*amazing reboot*" which tells us that some people liked the movie. 

* "*visually stunning, visual masterpiece, amazing visuals, beautifully animated, amazing animation*" these bigrams really describes the Spider-Man: Into the Spider-Verse well in my honest opinion. Even if you are not a Spider-Man fan but you like animations just go watch it. It was that beautiful. You can't really find anything negative within these bigrams.

* The most frequent word couple for No Way Home is "*fan service*" since the movie brought back former Spider-Men from the original trilogy and TASM.

### Reviews

```{r, fig.height=11}
bigrams <- review_data %>% 
  mutate(Review = str_to_lower(Review, locale = "en")) %>% 
  unnest_tokens(output = Bigram, input = Review, token = "ngrams", n = 2, to_lower = FALSE)

filtered_bigrams <- bigrams %>% 
  separate(Bigram, into = c("first", "second"), sep = " ") %>% 
  anti_join(stop_words, by = c("first" =  "word")) %>% 
  anti_join(stop_words, by = c("second" = "word")) 
  

filtered_bigrams %>% 
  unite(col = "Bigram", first, second, sep = " ") %>% 
  group_by(Bigram, Movie) %>% 
  count(sort = T, name = "Bigram_Count") %>% 
  ungroup() %>% 
  group_by(Movie) %>% 
  slice_max(order_by = Bigram_Count, n = 20,with_ties = FALSE) %>% 
  mutate(Bigram = reorder_within(Bigram, by = Bigram_Count, Movie)) %>% 
  ggplot(aes(Bigram_Count, Bigram, fill = Movie))+
  geom_col(show.legend = F)+
  facet_wrap(~Movie, scales = "free", strip.position = "right")+
  scale_fill_manual(values = spider_man_colors)+
  scale_y_reordered()+
  labs(x = NULL, y = NULL)
```

With review bigrams we can get more information about main characters and their respective actors/actresess. For instance in the original trilogy *kirsten dunst* played *mary jane* and *green goblin* was *willem dafoe*. The frequency of *green goblin* might give some clues about the performance of *willem dafoe* because for the other movies the most frequent bigram is either *peter parker* or the actor of *peter parker*. Of course without context we can't derive if *willem dafoe* was bad or good at his acting but I am going to tell that he was very good as *green goblin*. 

* *uncle ben* is a frequent word couple in all first movies of the series. However, in Homecoming *uncle ben* wasn't present but it is still among the most frequent bigrams. Reviewers were probably pointing that out since a Spider-Man origin story without Uncle Ben is a little bit unusual. 

### Bigram Graph

It is also possible to visualize word couples with a directed graph so that we can observe the connections between different words.

```{r, fig.height=14, fig.width=12}
filtered_bigrams %>% 
  unite(col = "Bigram", first, second, sep = " ") %>% 
  group_by(Bigram, Movie) %>% 
  count(sort = T, name = "Bigram_Count") %>% 
  separate(col = Bigram, into = c("first", "second"), sep = " ") %>% 
  filter(Bigram_Count > 40) %>% 
  graph_from_data_frame() %>% 
  ggraph(layout = "fr")+
  geom_node_point(size = 4, color = "#bf2431")+
  geom_edge_link(aes(edge_width = Bigram_Count, edge_alpha = Bigram_Count), edge_color = "#29329b",
                 arrow = arrow(length = unit(2, 'mm')))+
  geom_node_text(aes(label = name), repel = T, point.padding = unit(0.2, "lines"), vjust = 1)+
  theme_void()+
  theme(legend.position = "bottom")
```

## Trigrams

Lastly we are going to analyze the most frequent trigrams as well. As we add more words naturally we can understand the context more. Title trigrams won't be analyzed because titles are usually quite short. 

### Review

We usually see overlapping actor/actress name and character name in the trigrams so we are not getting anything new. There are some bits of information though.

* There are *mid credit* and *post credit scenes* in Far From Home.

* "*friend harry osborn, mentor tony stark*" give clues about Peter Parker's relationships.

* With "*aunt may's death*" we understand that Aunt May died in No Way Home.

```{r, fig.height=11}
trigrams <- review_data %>% 
  mutate(Review = str_to_lower(Review, locale = "en")) %>% 
  unnest_tokens(output = Trigram, input = Review, token = "ngrams", n = 3, to_lower = FALSE)

filtered_trigrams <- trigrams %>% 
  separate(Trigram, into = c("first", "second", "third"), sep = " ") %>% 
  anti_join(stop_words, by = c("first" =  "word")) %>% 
  anti_join(stop_words, by = c("second" = "word")) %>% 
  anti_join(stop_words, by = c("third" = "word"))
  

filtered_trigrams %>% 
  unite(col = "Trigram", first, second, third, sep = " ") %>% 
  group_by(Trigram, Movie) %>% 
  count(sort = T, name = "Trigram_Count") %>% 
  ungroup() %>% 
  group_by(Movie) %>% 
  slice_max(order_by = Trigram_Count, n = 20,with_ties = FALSE) %>% 
  mutate(Trigram = reorder_within(Trigram, by = Trigram_Count, Movie)) %>% 
  ggplot(aes(Trigram_Count, Trigram, fill = Movie))+
  geom_col(show.legend = F)+
  facet_wrap(~Movie, scales = "free", strip.position = "right")+
  scale_fill_manual(values = spider_man_colors)+
  scale_x_continuous(n.breaks = 3)+
  scale_y_reordered()+
  labs(x = NULL, y = NULL)
```




### Trigram Graph

```{r, fig.height=14, fig.width=12}
filtered_trigrams %>% 
  unite(col = "Trigram", first, second, third, sep = " ") %>% 
  group_by(Trigram, Movie) %>% 
  count(sort = T, name = "Trigram_Count") %>% 
  separate(col = Trigram, into = c("first", "second", "third"), sep = " ") %>% 
  filter(Trigram_Count > 10) %>% 
  graph_from_data_frame() %>% 
  ggraph(layout = "fr")+
  geom_node_point(size = 4, color = "#bf2431")+
  geom_edge_link(aes(edge_alpha = Trigram_Count, edge_width = Trigram_Count), edge_color = "#29329b",
                 arrow = arrow(length = unit(2, 'mm')))+
  geom_node_text(aes(label = name), repel = T, point.padding = unit(0.2, "lines"), vjust = 1)+
  theme_void()+
  theme(legend.position = "bottom")
```

## TF-IDF Analysis

So far we only counted words or word pairs in a given movie's review but those numbers were raw. They weren't relative to the the number of total words in a document. Additionally, words such as "*peter*" occurs in every movie so we know that the main guys is Peter. However, we might be interested in unique features of a given movie. That's where Term Frequency - Inverse Document Frequency comes in to the play. This metric helps us differentiate documents from each other. Since the word *peter* is present in every movie it's IDF is going to be "0". Thus, no matter how frequent this word is its importance is going to be zero.

Here I only analyzed Bigram TF-IDF because unigrams and trigrams were giving similar results.

## Bigram TF-IDF

Before plotting the highest TF-IDF scores let's plot the most frequent bigrams relative to all bigrams.

```{r, fig.height=11}
filtered_bigrams %>% 
  unite(col = "Bigram", first, second, sep = " ") %>% 
  group_by(Movie, Bigram) %>% 
  count() %>% 
  ungroup() %>% 
  filter(n > 1) %>% 
  bind_tf_idf(term = Bigram, document = Movie, n = n) %>% 
  group_by(Movie) %>% 
  slice_max(order_by = tf, n = 20, with_ties = F) %>% 
  mutate(Bigram = reorder_within(Bigram, tf, Movie)) %>% 
  ggplot(aes(tf, Bigram, fill = Movie))+
  geom_col(show.legend = F)+
  facet_wrap(~Movie, scales = "free", strip.position = "right")+
  scale_fill_manual(values = spider_man_colors)+
  scale_x_continuous(n.breaks = 3)+
  scale_y_reordered()+
  labs(y = NULL, x = "Term Frequency")
```

Kind of similar to the bigram counts we analyzed above. Let's now plot TF-IDF for each movie.

```{r, fig.height=11}
filtered_bigrams %>% 
  unite(col = "Bigram", first, second, sep = " ") %>% 
  group_by(Movie, Bigram) %>% 
  count() %>% 
  ungroup() %>% 
  filter(n > 2) %>% 
  bind_tf_idf(term = Bigram, document = Movie, n = n) %>% 
  group_by(Movie) %>% 
  slice_max(order_by = tf_idf, n = 20, with_ties = F) %>% 
  mutate(Bigram = reorder_within(Bigram, tf_idf, Movie)) %>% 
  ggplot(aes(tf_idf, Bigram, fill = Movie))+
  geom_col(show.legend = F)+
  facet_wrap(~Movie, scales = "free", strip.position = "right")+
  scale_fill_manual(values = spider_man_colors)+
  scale_x_continuous(n.breaks = 3)+
  scale_y_reordered()+
  labs(y = NULL, x = "TF-IDF")
```

This time we are only left with bigrams that are unique to their respective movies. 

* *green goblin* probably got lost because he is such a loved villain and probably got mentioned in every Spider-Man movie's reviews. Instead we see his actor *willem dafoe* in Spider-Man and No Way Home. Additionally, actors of villains of the Spider-Man are usually in top 3 bigrams. *rhys ifans* was the *lizard* in TASM. *micheal keaton* was *vulture* in Homecoming and *topher grace* was *venom* in Spider-Man 3.

* *emma stone* is unique to The Amazing Spider-Man series. She played *gwen stacy*.


## Sentiment Analysis

In this section  I am going to use lexicon based sentiment analysis. A lexicon is simply a dictionary which contains certain words. Different lexicons evaluate words differently. For instance "AFINN" gives words numeric values between -5 to 5. Another lexicon "EmoLex" categorizes words by their emotion values. These categories could be anger, joy, fear, etc.

I will filter out the word "*amazing*" from the lexicons because two of the movies that I analyze has the name of "The *Amazing* Spider-Man". We've seen above that the word *amazing* was very dominant for those movies. It is high likely that most of the reviewers weren't using that word to show their praises to the movie. Before we move on we should check if this is the case. First I am going to count the word *amazing* in the reviews with respect to the each movie then divide this number by the total reviews that each movie has.

Another problem word is *marvel*. Even though all of these movies are Marvel movies, that word was used many more times for Home series compared to other movies. In AFINN lexicon *marvel* is a positive word so I am leaving that out too.

This also shows that lexicon based approaches can cause problems when the context is different so caution is important.

```{r, fig.height=9}
Review_N <- review_data %>% 
  group_by(Movie) %>% 
  count(name = "Review_N")

p1 <- unigram_tokens %>% 
  anti_join(stop_words, by = c("Word" = "word")) %>% 
  filter(Word == "amazing") %>% 
  group_by(Movie, Word) %>% 
  count(sort = T) %>% 
  inner_join(Review_N, by = "Movie") %>% 
  mutate(prop = n / Review_N) %>% 
  ggplot(aes(prop, reorder(Movie, prop)))+
  geom_col(fill = "#29329b")+
  labs(x = "'Amazing' per Review", y = NULL)

p2 <- unigram_tokens %>% 
  anti_join(stop_words, by = c("Word" = "word")) %>% 
  filter(Word == "marvel") %>% 
  group_by(Movie, Word) %>% 
  count(sort = T) %>% 
  inner_join(Review_N, by = "Movie") %>% 
  mutate(prop = n / Review_N) %>% 
  ggplot(aes(prop, reorder(Movie, prop)))+
  geom_col(fill = "#29329b")+
  labs(x = "'Marvel' per Review", y = NULL)

p1 / p2
```

Just as suspected. Nearly every review of TASM and TASM 2 contains the word *amazing* once. We see that Homecoming is in the third place. This might be because the movie came after TASM series so maybe reviewers were comparing Homecoming with TASM.

The usage of *marvel* isn't as dramatic as *amazing*'s but I think removing it from the lexicon is still justified.

### Total Sentiment for Each Movie

```{r, fig.height=6}
afinn_filtered <- read_csv("../input/bing-nrc-afinn-lexicons/Afinn.csv") %>% 
  filter(word != "amazing" & word != "marvel")

Review_Sentiment <- unigram_tokens %>% 
  anti_join(stop_words, by = c("Word" = "word")) %>% 
  inner_join(afinn_filtered, by = c("Word" = "word")) %>% 
  group_by(Review_ID) %>%  
  summarise(Review_sentiment = sum(value)) 

Review_N <- review_data %>% 
  group_by(Movie) %>% 
  count(name = "Review_N")

review_data %>% 
  inner_join(Review_Sentiment, by = "Review_ID") %>% 
  group_by(Movie) %>% 
  summarise(Total_Sentiment = sum(Review_sentiment)) %>% 
  inner_join(Review_N, by = "Movie") %>% 
  mutate(Avg_Sentiment = Total_Sentiment / Review_N) %>% 
  ggplot(aes(Avg_Sentiment, reorder(Movie, Avg_Sentiment), fill = Avg_Sentiment > 0))+
  geom_col(show.legend = F)+
  scale_fill_manual(values = spider_man_colors)+
  scale_x_continuous(n.breaks = 7)+
  labs(y = NULL, x = "Sentiment per Review")
```

Since some movies were reviewed many more times than the others(more reviews mean more words, more words mean higher sentiment score) I divided the total sentiment by the total number of reviews that each movie got. I want to point out that before I filtered out the word *amazing*. TASM was the first in the graph. I know that Spider-Man 3 got a lot of hate when it was first released (we also observed that more recent reviews had higher ratings.) so it is not a surprise that its average sentiment score is negative. TASM 2 was also mostly received negatively by the fans and basically got cancelled so it makes sense that it takes the second place of worst average sentiment score. Average scores of other movies are kind of similar. Although I would expect No Way Home to be way higher on the list. 

### Sentiment Score for Each Rating

Is there any relationship between the sentiment score and the review rating? If I gave a movie a high rating I would certainly be reviewing it in a positive manner. Would you do the same? Let's check if there is indeed a relationship between these two variables.

```{r}
unigram_tokens %>% 
  inner_join(afinn_filtered, by = c("Word" = "word")) %>% 
  group_by(Movie, Review_ID) %>% 
  summarise(Total_Sentiment = sum(value)) %>% 
  inner_join(review_data %>% select(Review_ID, Rating), by = "Review_ID") %>% 
  ggplot(aes(Rating, Total_Sentiment))+
  geom_boxplot(aes(group = Rating), color = "#29329b")+
  geom_smooth(color = "#9d1a28")+
  facet_wrap(~Movie)+
  scale_x_continuous(n.breaks = 10)+
  coord_cartesian(ylim = c(-20, 65))+
  labs(y = "Sentiment Score")
```

This plot isn't enough for making inferences(many omitted variables) but it looks like Rating and Sentiment Score has a positive relationship. I also want to point out that I zoomed the plots a little bit so we can see smoother lines better(some dots are looking weirdly cut off.). For most movies, it actually peaks at rating 8 then decreases a little bit. 

We can also look at the problem from a different perspective. In the plot below we can see average sentiment scores with respect to the number of reviews. Even at Rating 6 the average sentiment of Spider-Man 3 reviews is negative. 

```{r, fig.height=15, fig.width=8}
Review_Sentiment <- unigram_tokens %>% 
  anti_join(stop_words, by = c("Word" = "word")) %>% 
  inner_join(afinn_filtered, by = c("Word" = "word")) %>% 
  group_by(Review_ID) %>%  
  summarise(Review_sentiment = sum(value)) 

Review_N <- review_data %>% 
  mutate(Rating = as.factor(Rating)) %>% 
  group_by(Movie, Rating) %>% 
  count(name = "Review_N")

review_data %>% 
  filter(!is.na(Rating)) %>% 
  mutate(Rating = as.factor(Rating)) %>% 
  inner_join(Review_Sentiment, by = "Review_ID") %>% 
  group_by(Movie, Rating) %>% 
  summarise(Total_Sentiment = sum(Review_sentiment)) %>% 
  inner_join(Review_N, by = c("Movie", "Rating")) %>% 
  mutate(Avg_Sentiment = Total_Sentiment / Review_N) %>% 
  ggplot(aes(Avg_Sentiment, reorder_within(Movie, Avg_Sentiment, Rating), fill = Avg_Sentiment > 0))+
  geom_col(show.legend = F)+
  scale_fill_manual(values = spider_man_colors)+
  scale_y_reordered()+
  facet_wrap(~Rating, scales = "free_y", ncol = 1,strip.position = "right")+
  labs(y = NULL, x = "Average Sentiment Score per Review")+
  theme(panel.spacing = unit(0, "cm"),
        strip.text.y = element_text(angle = 360))
```

### Sentiment Contribution by Each Word

We've seen the average sentiment scores for each movie. Do we know what words contributed to these scores? Let's find out.

```{r, fig.height=14}
unigram_tokens %>% 
  inner_join(afinn_filtered, by = c("Word" = "word")) %>% 
  group_by(Movie, Word) %>% 
  summarise(total_sent = sum(value),
            size = n()) %>%
  ungroup() %>% 
  group_by(Movie) %>% 
  slice_max(order_by = abs(total_sent),n = 30, with_ties = F) %>% 
  mutate(Word = reorder_within(Word, total_sent, Movie)) %>% 
  ggplot(aes(total_sent, Word, fill = total_sent >0))+
    geom_col(show.legend = F)+
    facet_wrap(~Movie, scales = "free")+
    scale_fill_manual(values = spider_man_colors)+
    scale_y_reordered()+
    labs(y = NULL, x = "Total Sentiment")
```

* Words like *bad* and *good* were the top contributors towards negative and positive sentiment. 

* We see the word *boring* contributed negatively for 4 movies. Namely, TASM, TASM 2, Homecoming and Far From Home. The movies maybe had some pacing problems and this was definitely voiced by reviewers. Personally, I found those movies boring too, especially the first halves of those movies.

* *disappointed* is on the plot of the Spider-Man 3. This word isn't present for other movies. It really shows the higher standards that Spider-Man 3 faced.

We can also plot the contributions of the words for each rating score. As you can see below as the review rating increases the variety of the positive words increases too.

```{r, fig.height=18, fig.width=8}
unigram_tokens %>% 
  filter(!is.na(Rating)) %>% 
  inner_join(afinn_filtered, by = c("Word" = "word")) %>% 
  group_by(Rating, Word) %>% 
  summarise(total_sent = sum(value),
            size = n()) %>%
  ungroup() %>% 
  group_by(Rating) %>% 
  slice_max(order_by = abs(total_sent),n = 30, with_ties = F) %>% 
  mutate(Word = reorder_within(Word, total_sent, Rating)) %>% 
  ggplot(aes(total_sent, Word, fill = total_sent >0))+
    geom_col(show.legend = F)+
    facet_wrap(~Rating, scales = "free", ncol = 2,strip.position = "right")+
    scale_fill_manual(values = spider_man_colors)+
    scale_y_reordered()+
    theme(strip.text.y = element_text(angle = 360))+
    labs(y = NULL, x = "Total Sentiment")
```


### Sentiment Score versus Review Helpfulness

We might expect a positive relationship between the helpfulness of a review and and its sentiment score because the way a review is written might change how it is received by readers. A more positively written review might attract more helpful votes even tough the reader doesn't agree with that particular review.

```{r}
unigram_tokens %>% 
  inner_join(afinn_filtered, by = c("Word" = "word")) %>% 
  group_by(Review_ID) %>% 
  summarise(Total_Sentiment = sum(value)) %>% 
  inner_join(review_data, by = "Review_ID") %>% 
  filter(Total_Vote > 1) %>% 
  ggplot(aes(Total_Sentiment, Helpfulness))+
  geom_point(color = "#29329b", alpha = 0.4)+
  geom_smooth(color = "#9d1a28", size = 1.2) +
  facet_wrap(~Movie, scales = "free")+
  labs(x = "Total Sentiment")
```

For some movies (Spider-Man and Spider-Man 2) our hypothesis might hold because there are strong positive relationships. There are also some weak positive relationships for a few movies. Homecoming and TASM have a flat line, meaning no effect. On Spider-Man 3's plot there is a weak negative relationship. This might be because the movie got so much heat that any positive review got its fair share of dislike buttons as well. 

### The Best and the Worst Reviews

Since we calculated an average sentiment score for each review we can check if scores really make sense contextually (Will we really get the feeling of positiveness or the opposite after reading the review.). To get representative worst and best reviews I set some conditions. Those reviews should get a certain amount of total votes, they shouldn't be short and their helpfulness score should be above %50.

```{r}
worst_reviews <- unigram_tokens %>% 
  inner_join(afinn_filtered, by = c("Word" = "word")) %>% 
  group_by(Movie, Review_ID) %>% 
  summarise(Total_Sentiment = sum(value),
            Word_Count = n()) %>% 
  inner_join(review_data %>% 
               select(Review_ID, Helpful_Vote, Total_Vote, Helpfulness),
             by = "Review_ID") %>% 
  mutate(Avg_Sent_per_Word = Total_Sentiment / Word_Count) %>% 
  filter(Avg_Sent_per_Word < 0) %>% 
  filter(Word_Count > 5 & Total_Vote > 25, Helpfulness > 0.5) %>% 
  slice_min(order_by = Avg_Sent_per_Word, n = 1) 

best_reviews <- unigram_tokens %>% 
  inner_join(afinn_filtered, by = c("Word" = "word")) %>% 
  group_by(Movie, Review_ID) %>% 
  summarise(Total_Sentiment = sum(value),
            Word_Count = n()) %>% 
  inner_join(review_data %>% 
               select(Review_ID, Helpful_Vote, Total_Vote, Helpfulness),
             by = "Review_ID") %>% 
  mutate(Avg_Sent_per_Word = Total_Sentiment / Word_Count) %>% 
  filter(Avg_Sent_per_Word > 0) %>% 
  filter(Word_Count > 10 & Total_Vote > 50, Helpfulness > 0.65) %>% 
  slice_max(order_by = Avg_Sent_per_Word, n = 1) 


# A function for printing reviews nicely
print_reviews <- function(IDs) {
  
  reviews <- review_data %>%
    filter(Review_ID %in% IDs) 
  
  cat(paste("\n",
                   "--- --- --- --- --- -- -- --    ", reviews$Movie, "    -- -- -- --- --- --- --- --- \n",
                   "\n Review Rating : ", reviews$Rating, "/10\n",
             "\n Helpful Votes/Total Votes : ", reviews$Helpful_Vote, "/", reviews$Total_Vote,
                   "\n\n",
             "<blockquote>", reviews$Review, "\n", "</blockquote>",
             "\n"))
}

```

Let's print some of the worst reviews for each movie according to my arbitrary conditions.


***


```{r, results='asis'}
print_reviews(worst_reviews$Review_ID)
```


***


The review of Spider-Man is a false positive here unfortunately. Looks like the reviewer only summarized the movie and because lots of bad stuff happens in an action superhero movie the review on the average got a negative sentiment score. He/she even rated 9! Other reviews make sense, they are indeed negative and their rating scores are very low. Note that there wasn't any review for Spider-Man 2 which suited my conditions.

Now it's time for good reviews. Those are longer than their counterparts so please bear with me.


***


```{r, results='asis'}
print_reviews(best_reviews$Review_ID)
```


***


Aside from Homecoming review I think average sentiment score did its job quite well. Reading these reviews gives a good idea about how those movies were received.

Ready for some fire? Let's also print some of the most negative and the most unhelpful reviews! I hope you like some flame.


***


```{r, results='asis'}
tasteless_reviews <- unigram_tokens %>% 
  inner_join(afinn_filtered, by = c("Word" = "word")) %>% 
  group_by(Movie, Review_ID) %>% 
  summarise(Total_Sentiment = sum(value),
            Word_Count = n()) %>% 
  inner_join(review_data %>% 
               select(Review_ID, Helpful_Vote, Total_Vote, Helpfulness),
             by = "Review_ID") %>% 
  mutate(Avg_Sent_per_Word = Total_Sentiment / Word_Count) %>% 
  filter(Avg_Sent_per_Word < 0) %>% 
  filter(Word_Count > 5 & Total_Vote > 25, Helpfulness < 0.2) %>% 
  slice_min(order_by = Avg_Sent_per_Word, n = 1) 

print_reviews(tasteless_reviews$Review_ID)
```

***

### Negated Words

When we were checking out the worst reviews we encountered this false positive. The writer of the review didn't actually say anything bad about the movie but we still got a negative sentiment score. 

***

```{r, results='asis'}
print_reviews(2210)
```

***

One of the reasons of the negative sentiment could be negation words. The reviewer said that the movie "was not boring" but unigram analysis unfortunately can't detect that. Let's now try to plot what words were negated most of the time.


```{r, fig.height=33}
negate_words <- c("not", "without", "no", "can't", "don't", "won't")


negated_words <- bigrams %>% 
  separate(Bigram, into = c("first", "second"), sep = " ") %>% 
  filter(first %in% negate_words) %>% 
  inner_join(read_csv("../input/bing-nrc-afinn-lexicons/Afinn.csv"), by = c("second" = "word"))


plot_negation_graphs <- function(Movie_Name, negated_words) {

negated_words %>% 
  group_by(Movie,first, second) %>% 
  summarise(total = sum(value)) %>% 
  ungroup() %>% 
  group_by(Movie, first) %>% 
  slice_max(abs(total), n = 10, with_ties = F) %>% 
  filter(Movie == Movie_Name) %>%
  mutate(second = reorder_within(second, total, first)) %>% 
  ggplot(aes(total, second, fill = total > 0))+
  geom_col(show.legend = F)+
  facet_wrap(~first, scales = "free")+
  scale_y_reordered()+
  scale_fill_manual(values = spider_man_colors)+
  ggtitle({{Movie_Name}})+
  labs(y = NULL, x = NULL)
  
}

map(unique(review_data$Movie), plot_negation_graphs, negated_words = negated_words) %>% 
  patchwork::wrap_plots(ncol = 1)
```

That's a big plot so I am leaving most of the exploration to the reader. 

# Topic Modelling

In the last section of this notebook we are going to use LDA in order to cluster our reviews. Latent Dirichlet allocation (LDA) could be considered as an unsupervised learning algorithm for text data. It clusters the documents based on the probability of certain words existing in a given topic. Kind of similar to k-Means clustering algorithm. In the code below I am preparing the sparse matrix that is needed to apply LDA. I also filtered out very rare words.

```{r}
LDA_Data <- unigram_tokens %>% 
  anti_join(stop_words, by = c("Word" = "word")) %>% 
  group_by(Movie, Word) %>% 
  count(name = "Word_Total") %>% 
  ungroup() %>% 
  filter(Word_Total > 10)

Sparse_Mat <- LDA_Data %>% 
  cast_dtm(Movie, Word, Word_Total)
```

I am going to use two models. The first model is going to have 4 clusters since in the data set we have 9 movies which came from 4 different sources and directors. We have the original trilogy, The Amazing Spider-Man series, Marvel Cinematic Universe Series and a Spider-Verse animation. I wonder if clustering would reflect this as well. Then for the second model I am going to create 9 clusters to see if individual movies are going to be classified correctly.

## Movie Series

Let's cluster and then plot the word probabilities for each topic. I will try to label these topics with my own knowledge. After that we are going to see the actual gamma values and check how the movies were classified.

```{r, fig.height=10}
movie_LDA_4 <- LDA(Sparse_Mat, k = 4, control = list(seed = 1993))

tidy(movie_LDA_4) %>% 
  group_by(topic) %>% 
  slice_max(beta, n = 40) %>% 
  ungroup() %>% 
  mutate(term = reorder_within(term, beta, topic)) %>% 
  ggplot(aes(beta, term, fill = as.factor(topic)))+
  geom_col(show.legend = F)+
  facet_wrap(~topic, scales = "free")+
  scale_y_reordered()+
  labs(y = NULL, x = "Probability of being a part of a Topic")+
  scale_fill_manual(values = spider_man_colors)
```

Here are my predictions about the topics:

1. We see words such as "*raimi, tobey, venom, sandman, goblin, black, suit eddie*". I think this topic represents the Original Trilogy with a heavy emphasis on Spider-Man 3.

2. "*tom, holland, mcu, home, tony*" represent the Marvel Cinematic Universe.

3. In The Amazing Spider-Man series there were "*gwen, stacy, andrew, garfield, lizard, webb, electro*" so this topic is TASM most likely.

4. This topic is a little bit diluted. There are elements from both Spider-Verse and the Original Trilogy.  It's heavy on the trilogy side though.  

Let's check the gamma values if my classifications were right.

```{r}
tidy(movie_LDA_4, matrix = "gamma") %>% 
  ggplot(aes(gamma, reorder_within(document, gamma, topic), fill = document))+
  geom_col(show.legend = F)+
  facet_wrap(~topic, scales = "free_y")+
  scale_fill_manual(values = spider_man_colors)+
  scale_y_reordered()+
  labs(y = NULL, x = "Probability of a Movie belonging to a Topic")
```

The first topic is kind of true but we are missing Spider-Man. The second topic is spot on, we also see a little bit of Spider-Verse there. The third topic classifies TASM series well. At last we can observe the confusion in the fourth topic. The original trilogy and Spider-Verse were mixed. It is kind of understandable because their villains and love affairs share the same characters like "dr ock, mary jane, aunt may" etc. 

## Individual Movies

This time I am increasing the number of clusters to 9 which is equal to the number of movies we have in the data set. The expectations is that every single topic represents a unique movie. I am going to list my guesses here too.

```{r, fig.height=14}
movie_LDA_9 <- LDA(Sparse_Mat, k = 9, control = list(seed = 1993))

tidy(movie_LDA_9) %>% 
  group_by(topic) %>% 
  slice_max(beta, n = 35) %>% 
  ungroup() %>% 
  mutate(term = reorder_within(term, beta, topic)) %>% 
  ggplot(aes(beta, term, fill = as.factor(topic)))+
  geom_col(show.legend = F)+
  facet_wrap(~topic, scales = "free",
             strip.position = "right")+
  scale_y_reordered()+
  labs(y = NULL, x = "Probability of being a part of a Topic")+
  scale_fill_manual(values = spider_man_colors)+
  theme(strip.text.y = element_text(angle = 360))
```

1. Spider-Man 3: "*venom, sandman, 3, black, suit*" 

2. TASM2: "*electro, gwen, goblin, foxx, rhino*"

3. Spider-Man: "*dunst, raimi, maguire, green, goblin, dafoe*"

4. Original Trilogy but nothing specific.

5. TASM: *lizard, reboot, connors, webb*"

6. Homecoming: "*homcoming, holland, vulture, tony, iron*" 

7. No Way Home: "*strange, mcu, tobey, andrew, tom*"

8. Far From Home: "*mysterio, endgame, mj, gyllenhaal, stark*" 

9. Spider-Verse: "*animation, miles, morales*" 

Let's see gamma values.

```{r, fig.height=11}
tidy(movie_LDA_9, matrix = "gamma") %>% 
  ggplot(aes(gamma, reorder_within(document, gamma, topic), fill = document))+
  geom_col(show.legend = F)+
  facet_wrap(~topic, scales = "free_y", ncol = 2)+
  scale_y_reordered()+
  labs(y = NULL, x = "Probability of a Movie belonging to a Topic")+
  scale_fill_manual(values = spider_man_colors)
```

My predictions were mostly okay except for the original trilogy. Looks like the model had a very hard time separating Spider-Man and Spider-Man 2. 

## Modelling with Bigrams

I want to see if using bigrams instead of unigrams would give more identification power to LDA.

```{r}
LDA_Data <- filtered_bigrams %>% 
  unite(col = "Bigram", first, second, sep = " ") %>% 
  group_by(Movie, Bigram) %>% 
  count(name = "Bigram_Total") %>% 
  ungroup() %>% 
  filter(Bigram_Total > 1)

Sparse_Mat <- LDA_Data %>% 
  cast_dtm(Movie, Bigram, Bigram_Total)
```

Just as I did with unigrams I am starting with 4 clusters.

```{r, fig.height=12}
movie_LDA_4_bigram <- LDA(Sparse_Mat, k = 4, control = list(seed = 1993))

tidy(movie_LDA_4_bigram) %>% 
  group_by(topic) %>% 
  slice_max(beta, n = 40) %>% 
  ungroup() %>% 
  mutate(term = reorder_within(term, beta, topic)) %>% 
  ggplot(aes(beta, term, fill = as.factor(topic)))+
  geom_col(show.legend = F)+
  facet_wrap(~topic, scales = "free")+
  scale_y_reordered()+
  labs(y = NULL, x = "Probability of being a part of a Topic")+
  scale_fill_manual(values = spider_man_colors)
```

As with unigrams Marvel Cinematic Universe movies got classified well in the first topic. For some reason Into the Spider-Verse went to the topic of TASM series. This wasn't the case with unigrams. Lastly, the model had the same trouble clustering the original trilogy, I would expect that Spider-Man, Spider-Man 2 and Spider-Man 3 go into the same topic but it' didn't happen with bigrams neither. 

```{r}
tidy(movie_LDA_4_bigram, matrix = "gamma") %>% 
  ggplot(aes(gamma, reorder_within(document, gamma, topic), fill = document))+
  geom_col(show.legend = F)+
  facet_wrap(~topic, scales = "free_y")+
  scale_fill_manual(values = spider_man_colors)+
  scale_y_reordered()+
  labs(y = NULL, x = "Probability of a Movie belonging to a Topic")
```

Let's now increase the cluster size to 9 to see that if bigrams worked well clustering separate movies.

```{r, fig.height=22}
movie_LDA_9_bigram <- LDA(Sparse_Mat, k = 9, control = list(seed = 1993))

tidy(movie_LDA_9_bigram) %>% 
  group_by(topic) %>% 
  slice_max(beta, n = 35) %>% 
  ungroup() %>% 
  mutate(term = reorder_within(term, beta, topic)) %>% 
  ggplot(aes(beta, term, fill = as.factor(topic)))+
  geom_col(show.legend = F)+
  facet_wrap(~topic, scales = "free", strip.position = "right", ncol = 2)+
  scale_y_reordered()+
  labs(y = NULL, x = "Probability of being a part of a Topic")+
  scale_fill_manual(values = spider_man_colors)+
  theme(strip.text.y = element_text(angle = 360))
```

OK. Turns out bigrams are giving more power to LDA. Now original trilogy looks more separated.

```{r, fig.height=10}
tidy(movie_LDA_9_bigram, matrix = "gamma") %>% 
  ggplot(aes(gamma, reorder_within(document, gamma, topic), fill = document))+
  geom_col(show.legend = F)+
  facet_wrap(~topic, scales = "free_y", ncol = 2)+
  scale_fill_manual(values = spider_man_colors)+
  scale_y_reordered()+
  labs(y = NULL, x = "Probability of a Movie belonging to a Topic")
```


# Conclusion

To be honest I don't think there are anything to conclude in this notebook but I have two remarks:

1. Knowing about the context when text mining is very important because without the context it isn't possible to identify potential biases. This becomes much more important in supervised learning.

2. I think a custom stop words list for the reviews of the Spider-Man Movies is needed in order to understand the reactions better. Since the most important words were cluttered with character or actor/actress names it was kind of hard to get reaction words such as "disappointed".
























